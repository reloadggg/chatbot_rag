from app.settings import settings
from app.document_processor import document_processor
from app.gemini_handler import gemini_handler
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_qdrant import Qdrant
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.language_models import BaseLanguageModel
from langchain_core.embeddings import Embeddings
import chromadb
from qdrant_client import QdrantClient

class RAGPipeline:
    def __init__(self):
        self.settings = settings
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.chain = None
        self.provider = None
        self._initialize_components()
    
    def _initialize_components(self):
        try:
            print(f"ğŸŒ å½“å‰ç¯å¢ƒ: {settings.env}")
            print(f"ğŸ’¡ ä½¿ç”¨åµŒå…¥æ¨¡å‹: {settings.embedding_model}")
            print(f"ğŸ§  ä½¿ç”¨è¯­è¨€æ¨¡å‹: {settings.llm_model}")
            print(f"ğŸ”— æä¾›å•†: {settings.llm_provider}")
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self.embeddings = self._create_embeddings()
            if not self.embeddings:
                raise Exception("æ— æ³•åˆ›å»ºåµŒå…¥æ¨¡å‹")
            
            # åˆå§‹åŒ–å‘é‡å­˜å‚¨
            self.vectorstore = self._create_vectorstore()
            print("âœ… å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
            self.llm = self._create_llm()
            if not self.llm:
                raise Exception("æ— æ³•åˆ›å»ºè¯­è¨€æ¨¡å‹")
            
            # åˆ›å»ºRAGé“¾
            self._create_rag_chain()
            print("âœ… RAGç®¡é“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _create_embeddings(self) -> Embeddings:
        """åˆ›å»ºåµŒå…¥æ¨¡å‹"""
        try:
            if settings.embedding_provider == "gemini" and gemini_handler.is_available():
                embeddings = gemini_handler.create_embeddings()
                if embeddings:
                    self.provider = "gemini"
                    return embeddings
            
            # é»˜è®¤ä½¿ç”¨OpenAI
            embeddings = OpenAIEmbeddings(
                model=self.settings.embedding_model,
                api_key=self.settings.embedding_api_key,
                base_url=self.settings.embedding_base_url or None
            )
            self.provider = "openai"
            return embeddings
            
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
            raise
    
    def _create_vectorstore(self):
        """åˆ›å»ºå‘é‡å­˜å‚¨"""
        try:
            if self.settings.vector_db == "chroma":
                return Chroma(
                    collection_name="knowledge_base",
                    embedding_function=self.embeddings,
                    persist_directory=self.settings.vector_db_path
                )
            else:  # qdrant
                client = QdrantClient(
                    url=self.settings.qdrant_url,
                    api_key=self.settings.qdrant_api_key
                )
                return Qdrant(
                    client=client,
                    collection_name="knowledge_base",
                    embeddings=self.embeddings
                )
        except Exception as e:
            print(f"âŒ å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _create_llm(self) -> BaseLanguageModel:
        """åˆ›å»ºè¯­è¨€æ¨¡å‹"""
        try:
            if settings.llm_provider == "gemini" and gemini_handler.is_available():
                llm = gemini_handler.create_llm()
                if llm:
                    return llm
            
            # é»˜è®¤ä½¿ç”¨OpenAI
            llm = ChatOpenAI(
                model=self.settings.llm_model,
                api_key=self.settings.llm_api_key,
                base_url=self.settings.llm_base_url or None,
                temperature=self.settings.temperature,
                max_tokens=self.settings.max_tokens
            )
            return llm
            
        except Exception as e:
            print(f"âŒ è¯­è¨€æ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
            raise
    
    def _create_rag_chain(self):
        """åˆ›å»ºRAGé“¾"""
        template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š
        
        ä¸Šä¸‹æ–‡ï¼š
        {context}
        
        é—®é¢˜ï¼š{question}
        
        è¯·æä¾›å‡†ç¡®ã€ç®€æ´çš„ä¸­æ–‡å›ç­”ã€‚"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        self.chain = (
            {"context": self.vectorstore.as_retriever(search_kwargs={"k": self.settings.top_k}), "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
    
    def add_documents(self, documents):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        try:
            print(f"ğŸ“š æ­£åœ¨æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°å‘é‡æ•°æ®åº“...")
            
            # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
            self.vectorstore.add_documents(documents)
            
            print(f"âœ… æ–‡æ¡£æ·»åŠ æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£æ·»åŠ å¤±è´¥: {str(e)}")
            return False
    
    def query(self, question: str) -> str:
        """å•æ¬¡æŸ¥è¯¢"""
        try:
            print(f"ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸ä¼¼æ®µè½...")
            
            if self.provider == "gemini":
                # Geminiçš„ç‰¹æ®Šå¤„ç†å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
                pass
            
            answer = self.chain.invoke(question)
            print(f"âœ… å·²ç”Ÿæˆå›ç­”")
            return answer
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def stream_query(self, question: str):
        """æµå¼æŸ¥è¯¢"""
        try:
            print(f"ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸ä¼¼æ®µè½...")
            print(f"ğŸ§  æ­£åœ¨è°ƒç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”...")
            
            if self.provider == "gemini":
                # Geminiçš„ç‰¹æ®Šå¤„ç†å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
                pass
            
            for chunk in self.chain.stream(question):
                yield chunk
            print(f"âœ… æµå¼å›ç­”ç”Ÿæˆå®Œæˆ")
        except Exception as e:
            print(f"âŒ æµå¼æŸ¥è¯¢å¤±è´¥: {str(e)}")
            yield f"æŠ±æ­‰ï¼Œå¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def get_document_stats(self) -> dict:
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–å‘é‡å­˜å‚¨ä¸­çš„æ–‡æ¡£æ•°é‡
            if hasattr(self.vectorstore, '_collection'):
                count = self.vectorstore._collection.count()
            else:
                # é€šè¿‡æœç´¢ç©ºæŸ¥è¯¢æ¥ä¼°ç®—
                results = self.vectorstore.similarity_search("", k=1)
                count = len(results) if results else 0
            
            return {
                "document_count": count,
                "vector_db": self.settings.vector_db,
                "embedding_model": self.settings.embedding_model,
                "llm_model": self.settings.llm_model,
                "llm_provider": self.provider or self.settings.llm_provider,
                "status": "active"
            }
        except Exception as e:
            print(f"âŒ è·å–æ–‡æ¡£ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {
                "document_count": 0,
                "error": str(e),
                "status": "error"
            }
    
    def is_gemini_available(self) -> bool:
        """æ£€æŸ¥Geminiæ˜¯å¦å¯ç”¨"""
        return gemini_handler.is_available()

# å…¨å±€RAGç®¡é“å®ä¾‹
rag_pipeline = RAGPipeline()
