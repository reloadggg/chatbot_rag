from app.settings import settings
from app.document_processor import document_processor
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_qdrant import Qdrant
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import chromadb
from qdrant_client import QdrantClient

class RAGPipeline:
    def __init__(self):
        self.settings = settings
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.chain = None
        self._initialize_components()
    
    def _initialize_components(self):
        try:
            print(f"ğŸŒ å½“å‰ç¯å¢ƒ: {settings.env}")
            print(f"ğŸ’¡ ä½¿ç”¨åµŒå…¥æ¨¡å‹: {settings.embedding_model}")
            print(f"ğŸ§  ä½¿ç”¨è¯­è¨€æ¨¡å‹: {settings.llm_model}")
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self.embeddings = OpenAIEmbeddings(
                model=self.settings.embedding_model,
                api_key=self.settings.embedding_api_key,
                base_url=self.settings.__dict__.get('EMBEDDING_BASE_URL', None)
            )
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åˆå§‹åŒ–å‘é‡å­˜å‚¨
            if self.settings.vector_db == "chroma":
                self.vectorstore = Chroma(
                    collection_name="knowledge_base",
                    embedding_function=self.embeddings,
                    persist_directory=self.settings.vector_db_path
                )
            else:  # qdrant
                client = QdrantClient(
                    url=self.settings.qdrant_url,
                    api_key=self.settings.qdrant_api_key
                )
                self.vectorstore = Qdrant(
                    client=client,
                    collection_name="knowledge_base",
                    embeddings=self.embeddings
                )
            print("âœ… å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
            self.llm = ChatOpenAI(
                model=self.settings.llm_model,
                api_key=self.settings.llm_api_key,
                base_url=self.settings.__dict__.get('LLM_BASE_URL', None),
                temperature=self.settings.temperature,
                max_tokens=self.settings.max_tokens
            )
            print("âœ… è¯­è¨€æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åˆ›å»ºRAGé“¾
            self._create_rag_chain()
            print("âœ… RAGç®¡é“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _create_rag_chain(self):
        template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š
        
        ä¸Šä¸‹æ–‡ï¼š
        {context}
        
        é—®é¢˜ï¼š{question}
        
        è¯·æä¾›å‡†ç¡®ã€ç®€æ´çš„ä¸­æ–‡å›ç­”ã€‚"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        self.chain = (
            {"context": self.vectorstore.as_retriever(search_kwargs={"k": self.settings.top_k}), "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
    
    def add_documents(self, documents):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        try:
            print(f"ğŸ“š æ­£åœ¨æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°å‘é‡æ•°æ®åº“...")
            
            # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
            self.vectorstore.add_documents(documents)
            
            print(f"âœ… æ–‡æ¡£æ·»åŠ æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£æ·»åŠ å¤±è´¥: {str(e)}")
            return False
    
    def query(self, question: str) -> str:
        """å•æ¬¡æŸ¥è¯¢"""
        try:
            print(f"ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸ä¼¼æ®µè½...")
            answer = self.chain.invoke(question)
            print(f"âœ… å·²ç”Ÿæˆå›ç­”")
            return answer
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def stream_query(self, question: str):
        """æµå¼æŸ¥è¯¢"""
        try:
            print(f"ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸ä¼¼æ®µè½...")
            print(f"ğŸ§  æ­£åœ¨è°ƒç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”...")
            for chunk in self.chain.stream(question):
                yield chunk
            print(f"âœ… æµå¼å›ç­”ç”Ÿæˆå®Œæˆ")
        except Exception as e:
            print(f"âŒ æµå¼æŸ¥è¯¢å¤±è´¥: {str(e)}")
            yield f"æŠ±æ­‰ï¼Œå¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def get_document_stats(self) -> dict:
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–å‘é‡å­˜å‚¨ä¸­çš„æ–‡æ¡£æ•°é‡
            if hasattr(self.vectorstore, '_collection'):
                count = self.vectorstore._collection.count()
            else:
                # é€šè¿‡æœç´¢ç©ºæŸ¥è¯¢æ¥ä¼°ç®—
                results = self.vectorstore.similarity_search("", k=1)
                count = len(results) if results else 0
            
            return {
                "document_count": count,
                "vector_db": self.settings.vector_db,
                "embedding_model": self.settings.embedding_model,
                "status": "active"
            }
        except Exception as e:
            print(f"âŒ è·å–æ–‡æ¡£ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {
                "document_count": 0,
                "error": str(e),
                "status": "error"
            }

# å…¨å±€RAGç®¡é“å®ä¾‹
rag_pipeline = RAGPipeline()