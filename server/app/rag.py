from typing import Any, Dict, Optional, Tuple

from app.settings import settings
from app.gemini_handler import gemini_handler
from app.user_config import user_config_manager
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_qdrant import Qdrant
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.language_models import BaseLanguageModel
from langchain_core.embeddings import Embeddings
import chromadb
from qdrant_client import QdrantClient

class RAGPipeline:
    def __init__(self):
        self.settings = settings
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.chain = None
        self.provider = None
        self._prompt_template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

        ä¸Šä¸‹æ–‡ï¼š
        {context}

        é—®é¢˜ï¼š{question}

        è¯·æä¾›å‡†ç¡®ã€ç®€æ´çš„ä¸­æ–‡å›ç­”ã€‚"""
        self._initialize_components()
    
    def _initialize_components(self):
        try:
            print(f"ğŸŒ å½“å‰ç¯å¢ƒ: {settings.env}")
            print(f"ğŸ’¡ ä½¿ç”¨åµŒå…¥æ¨¡å‹: {settings.embedding_model}")
            print(f"ğŸ§  ä½¿ç”¨è¯­è¨€æ¨¡å‹: {settings.llm_model}")
            print(f"ğŸ”— æä¾›å•†: {settings.llm_provider}")
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self.embeddings = self._create_embeddings()
            if not self.embeddings:
                raise Exception("æ— æ³•åˆ›å»ºåµŒå…¥æ¨¡å‹")
            
            # åˆå§‹åŒ–å‘é‡å­˜å‚¨
            self.vectorstore = self._create_vectorstore()
            print("âœ… å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
            self.llm = self._create_llm()
            if not self.llm:
                raise Exception("æ— æ³•åˆ›å»ºè¯­è¨€æ¨¡å‹")
            
            # åˆ›å»ºRAGé“¾
            self._create_rag_chain()
            print("âœ… RAGç®¡é“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _create_embeddings(self) -> Embeddings:
        """åˆ›å»ºåµŒå…¥æ¨¡å‹"""
        try:
            if settings.embedding_provider == "gemini" and gemini_handler.is_available():
                embeddings = gemini_handler.create_embeddings()
                if embeddings:
                    self.provider = "gemini"
                    return embeddings
            
            # é»˜è®¤ä½¿ç”¨OpenAI
            embeddings = OpenAIEmbeddings(
                model=self.settings.embedding_model,
                api_key=self.settings.embedding_api_key,
                base_url=self.settings.embedding_base_url or None
            )
            self.provider = "openai"
            return embeddings
            
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
            raise
    
    def _create_vectorstore(self):
        """åˆ›å»ºå‘é‡å­˜å‚¨"""
        try:
            if self.settings.vector_db == "chroma":
                return Chroma(
                    collection_name="knowledge_base",
                    embedding_function=self.embeddings,
                    persist_directory=self.settings.vector_db_path
                )
            else:  # qdrant
                client = QdrantClient(
                    url=self.settings.qdrant_url,
                    api_key=self.settings.qdrant_api_key
                )
                return Qdrant(
                    client=client,
                    collection_name="knowledge_base",
                    embeddings=self.embeddings
                )
        except Exception as e:
            print(f"âŒ å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _create_llm(self) -> BaseLanguageModel:
        """åˆ›å»ºè¯­è¨€æ¨¡å‹"""
        try:
            if settings.llm_provider == "gemini" and gemini_handler.is_available():
                llm = gemini_handler.create_llm()
                if llm:
                    return llm
            
            # é»˜è®¤ä½¿ç”¨OpenAI
            llm = ChatOpenAI(
                model=self.settings.llm_model,
                api_key=self.settings.llm_api_key,
                base_url=self.settings.llm_base_url or None,
                temperature=self.settings.temperature,
                max_tokens=self.settings.max_tokens
            )
            return llm
            
        except Exception as e:
            print(f"âŒ è¯­è¨€æ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
            raise
    
    def _build_chain(self, llm: BaseLanguageModel):
        """æ ¹æ®æŒ‡å®šLLMåˆ›å»ºæ–°çš„RAGé“¾"""
        prompt = ChatPromptTemplate.from_template(self._prompt_template)
        return (
            {"context": self.vectorstore.as_retriever(search_kwargs={"k": self.settings.top_k}), "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

    def _create_rag_chain(self):
        """åˆ›å»ºé»˜è®¤RAGé“¾"""
        self.chain = self._build_chain(self.llm)
    
    def add_documents(self, documents):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        try:
            print(f"ğŸ“š æ­£åœ¨æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µåˆ°å‘é‡æ•°æ®åº“...")
            
            # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
            self.vectorstore.add_documents(documents)
            
            print(f"âœ… æ–‡æ¡£æ·»åŠ æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£æ·»åŠ å¤±è´¥: {str(e)}")
            return False
    
    def _resolve_chain(self, config: Optional[Dict[str, Any]]) -> Tuple[Any, str]:
        """æ ¹æ®ç”¨æˆ·é…ç½®é€‰æ‹©åˆé€‚çš„RAGé“¾å’Œæä¾›å•†ä¿¡æ¯"""
        provider = self.provider or self.settings.llm_provider

        if not config:
            return self.chain, provider

        try:
            config_obj = user_config_manager.create_user_config(config)
            llm = user_config_manager.create_llm(config_obj)
            provider = config_obj.llm_provider or provider
            chain = self._build_chain(llm)
            return chain, provider
        except Exception as e:
            print(f"âš ï¸  æ— æ³•æ ¹æ®ç”¨æˆ·é…ç½®åˆ›å»ºä¸“å±RAGé“¾ï¼Œå›é€€åˆ°é»˜è®¤é“¾: {str(e)}")
            return self.chain, provider

    def query(self, question: str, config: Optional[Dict[str, Any]] = None) -> Tuple[str, str]:
        """å•æ¬¡æŸ¥è¯¢"""
        try:
            print(f"ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸ä¼¼æ®µè½...")
            chain, provider = self._resolve_chain(config)

            answer = chain.invoke(question)
            print(f"âœ… å·²ç”Ÿæˆå›ç­”")
            return answer, provider
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")
            provider = (config or {}).get("llm_provider") or self.provider or self.settings.llm_provider
            return f"æŠ±æ­‰ï¼Œå¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}", provider

    def stream_query(self, question: str, config: Optional[Dict[str, Any]] = None):
        """æµå¼æŸ¥è¯¢"""
        try:
            print(f"ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸ä¼¼æ®µè½...")
            print(f"ğŸ§  æ­£åœ¨è°ƒç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”...")

            chain, _ = self._resolve_chain(config)

            for chunk in chain.stream(question):
                yield chunk
            print(f"âœ… æµå¼å›ç­”ç”Ÿæˆå®Œæˆ")
        except Exception as e:
            print(f"âŒ æµå¼æŸ¥è¯¢å¤±è´¥: {str(e)}")
            yield f"æŠ±æ­‰ï¼Œå¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def get_document_stats(self) -> dict:
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–å‘é‡å­˜å‚¨ä¸­çš„æ–‡æ¡£æ•°é‡
            if hasattr(self.vectorstore, '_collection'):
                count = self.vectorstore._collection.count()
            else:
                # é€šè¿‡æœç´¢ç©ºæŸ¥è¯¢æ¥ä¼°ç®—
                results = self.vectorstore.similarity_search("", k=1)
                count = len(results) if results else 0
            
            return {
                "document_count": count,
                "vector_db": self.settings.vector_db,
                "embedding_model": self.settings.embedding_model,
                "llm_model": self.settings.llm_model,
                "llm_provider": self.provider or self.settings.llm_provider,
                "status": "active"
            }
        except Exception as e:
            print(f"âŒ è·å–æ–‡æ¡£ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {
                "document_count": 0,
                "error": str(e),
                "status": "error"
            }
    
    def is_gemini_available(self) -> bool:
        """æ£€æŸ¥Geminiæ˜¯å¦å¯ç”¨"""
        return gemini_handler.is_available()

    def delete_documents(self, file_id: str) -> bool:
        """ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰ç‰‡æ®µ"""
        if not self.vectorstore or not hasattr(self.vectorstore, "delete"):
            return False

        try:
            self.vectorstore.delete(where={"file_id": file_id})
            print(f"ğŸ§¹ å·²ä»å‘é‡åº“åˆ é™¤ file_id={file_id} çš„æ–‡æ¡£ç‰‡æ®µ")
            return True
        except Exception as e:
            print(f"âš ï¸  åˆ é™¤å‘é‡åº“ä¸­æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False

# å…¨å±€RAGç®¡é“å®ä¾‹
rag_pipeline = RAGPipeline()
